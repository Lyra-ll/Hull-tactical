# create_features.py
# =================================================================
# ç»ˆæç‰¹å¾å·¥ç¨‹è„šæœ¬ V2.0 (ç»Ÿä¸€æ—¶ç©ºä¿®å¤ç‰ˆ)
# =================================================================
import pandas as pd
import time
import argparse
import warnings
import numpy as np

warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)

# Time-decay parameters
USE_TIME_DECAY = True
TIME_DECAY_HALF_LIFE = 365  # approximately one year
TIME_DECAY_COLUMN = 'date_id'

# --- æ‰‹å·¥ç‰¹å¾ç”Ÿæˆé€»è¾‘ (æ‚¨çš„â€œç‰¹å¾å…µå·¥å‚â€) ---
def create_manual_features(df, top_n_base_features):
    #æ¥æ”¶ä¸€ä¸ªdataframeå’Œä¹‹å‰å†³å®šçš„æœ€ä¼˜ç§€çš„åŸºç¡€ç‰¹å¾ï¼Œæ¥ç”Ÿäº§æ‰‹å·¥ç‰¹å¾
    """
    æ ¹æ®ç»™å®šçš„åŸºç¡€ç‰¹å¾åˆ—è¡¨ï¼Œæ‰¹é‡ç”Ÿäº§è¡ç”Ÿç‰¹å¾ã€‚
    """
    print(f"\n--- [å…µå·¥å‚] å¼€å§‹åŸºäº {len(top_n_base_features)} ä¸ªæ ¸å¿ƒç‰¹å¾ç”Ÿäº§æ‰‹å·¥ç‰¹å¾ ---")
    
    df_eng = df.copy()
    #åˆ›é€ å‰¯æœ¬é¿å…ä¿®æ”¹åŸæ•°æ®
    
    # æ ¸å¿ƒå‚æ•°
    horizons = [1, 3, 5]
    #å®šä¹‰äº†ç”¨æ¥ç”Ÿäº§æ—¥æœŸèŒƒå›´å·®å¼‚çš„å‚æ•°
    windows = [5, 10]
    #å®šä¹‰äº†ç”¨æ¥ç”Ÿäº§æ»šåŠ¨çª—å£èŒƒå›´çš„å‚æ•°

    for col in top_n_base_features:
        # 1. åŠ¨é‡ (Momentum) ç‰¹å¾
        for h in horizons:
            df_eng[f'{col}_diff{h}'] = df_eng[col].diff(h)
            #è®¡ç®—ä¸è¿‡å»hå¤©çš„å·®å€¼
        # 2. æ³¢åŠ¨/è¶‹åŠ¿ (Volatility/Trend) ç‰¹å¾
        for w in windows:
            df_eng[f'{col}_rol_mean_{w}'] = df_eng[col].rolling(window=w).mean()
            #è®¡ç®—wæ—¶é—´çª—å£é‡Œçš„å‡å€¼
            df_eng[f'{col}_rol_std_{w}'] = df_eng[col].rolling(window=w).std()
            #è®¡ç®—wæ—¶é—´çª—å£é‡Œçš„æ ‡å‡†å·®
            #æ ‡å‡†å·®è¶Šå¤§ï¼Œå¸‚åœºæƒ…ç»ªè¶Šä¸ç¨³å®š
    print(f"  > âœ… æ‰‹å·¥ç‰¹å¾ç”Ÿäº§å®Œæˆã€‚")
    return df_eng

def apply_time_decay_factor(df, weight_col):
    """Apply exponential time-decay on sample weights based on a date column."""
    if not USE_TIME_DECAY:
        return df
    if TIME_DECAY_COLUMN not in df.columns:
        print(f"    > Warning: column '{TIME_DECAY_COLUMN}' not found; skip time decay.")
        return df
    if TIME_DECAY_HALF_LIFE <= 0:
        print("    > Warning: TIME_DECAY_HALF_LIFE <= 0; skip time decay.")
        return df

    date_series = df[TIME_DECAY_COLUMN]
    distance = None

    if pd.api.types.is_datetime64_any_dtype(date_series):
        max_point = date_series.max()
        distance = (max_point - date_series).dt.days
    else:
        parsed = pd.to_datetime(date_series, errors='coerce')
        if parsed.notna().any():
            max_point = parsed.max()
            distance = (max_point - parsed).dt.days
        else:
            numeric = pd.to_numeric(date_series, errors='coerce')
            if numeric.notna().any():
                max_point = numeric.max()
                distance = max_point - numeric
            else:
                print("    > Warning: unable to parse time column; skip time decay.")
                return df

    distance = pd.Series(distance).fillna(distance.median()).astype(float)
    decay = np.power(0.5, distance / TIME_DECAY_HALF_LIFE)

    df[weight_col] = df[weight_col] * decay.values
    df[f"{weight_col}_time_decay"] = decay.values
    print(f"    > Applied time decay (half-life={TIME_DECAY_HALF_LIFE} days); recent rows receive higher weight.")
    return df





def create_safe_sample_weights(df, resp_columns):
    """
    [å† å†›ç­–ç•¥ä¿®å¤ç‰ˆ] åŸºäºæœªæ¥å¤šä¸ªåŸå§‹æ”¶ç›Šåˆ—çš„ç»å¯¹å€¼ä¹‹å’Œï¼Œåˆ›å»ºæ ·æœ¬æƒé‡ã€‚
    resp_columnså°±æ˜¯resp_1d, resp_3dç­‰ç­‰
    """
    print("\n--- [æƒé‡å·¥å‚] å¼€å§‹ç”Ÿæˆå† å†›ç­–ç•¥çš„æ ·æœ¬æƒé‡ ---")
    df_with_weights = df.copy()

    if not all(col in df_with_weights.columns for col in resp_columns):
        #æ£€æŸ¥æ˜¯å¦æ‰€æœ‰çš„ä¾‹å¦‚resp_1dç­‰å­˜åœ¨äºdf_with_weights.
        print(f"    > âŒ é”™è¯¯: æƒé‡ç”Ÿæˆéœ€è¦æ‰€æœ‰ resp åˆ—: {resp_columns}ã€‚")
        df_with_weights['sample_weight'] = 1.0
        return df_with_weights

    df_with_weights['sample_weight'] = df_with_weights[resp_columns].abs().sum(axis=1)
    #ä»dfé‡Œé¢æ‰¾å‡ºresp columnsé‚£å‡ è¡Œçš„ç»å¯¹å€¼åŠ èµ·æ¥ï¼Œèµ‹å€¼ç»™
    # --- [è­¦å‘Šä¿®å¤] ---
    # æ—§çš„å†™æ³•: df_with_weights['sample_weight'].fillna(0, inplace=True)
    # æ–°çš„ã€æ›´æ¨èçš„å†™æ³•:
    df_with_weights['sample_weight'] = df_with_weights['sample_weight'].fillna(0)
    # --- [ä¿®å¤ç»“æŸ] ---
    df_with_weights = apply_time_decay_factor(df_with_weights, 'sample_weight')

    print("  > âœ… å·²ç”Ÿæˆ 'sample_weight' åˆ— (åŸºäºå† å†›ç­–ç•¥)ã€‚")
    return df_with_weights


# --- å¤šç›®æ ‡åˆ—ç”Ÿæˆé€»è¾‘ (æ‚¨çš„â€œå¼¹è¯ç»„è£…çº¿â€) ---
# [V3.0 - ç»ˆææ­£ç¡®æ€§ä¿®å¤ç‰ˆ]
def create_multi_horizon_targets(df, processing_mode):
    """
    [V5.0 - ç»ˆæå…¼å®¹ä¿®å¤ç‰ˆ] + [V6.1 DLSå…³é”®ä¿®å¤ç‰ˆ]
    ä¸ºæ•°æ®é›†åˆ›é€ å¤šä¸ªæ—¶é—´å°ºåº¦çš„æœªæ¥æ”¶ç›Šç›®æ ‡ã€‚
    æ–°å¢äº†â€œåŠ¨æ€å¹³æ»‘æ ‡ç­¾ (DLS)â€ä½œä¸ºæ–°çš„ç›‘ç£ä¿¡å·ï¼Œå¹¶ä¿®å¤äº†actionåˆ—ä¸¢å¤±çš„BUGã€‚
    """
    print(f"\n--- [ç»„è£…çº¿ V6.1] å¼€å§‹æ·»åŠ å¤šå°ºåº¦ç›®æ ‡åˆ— (DLSä¿®å¤ç‰ˆ) ---")
    df_with_targets = df.copy()
    
    TARGET_HORIZONS = [1, 3, 5]

    source_return_col = None
    if processing_mode == 'train':
        if 'forward_returns' in df_with_targets.columns:
            source_return_col = 'forward_returns'
            print(f"  > [è®­ç»ƒé›†æ¨¡å¼] ä½¿ç”¨ '{source_return_col}' ä½œä¸ºæºã€‚")
        else:
            print(f"  > âŒ è‡´å‘½é”™è¯¯: è®­ç»ƒæ¨¡å¼ä¸‹æœªæ‰¾åˆ° 'forward_returns' åˆ—ã€‚")
            return df
    elif processing_mode == 'test':
        if 'lagged_forward_returns' in df_with_targets.columns:
            source_return_col = 'lagged_forward_returns'
            print(f"  > [æµ‹è¯•é›†æ¨¡å¼] ä½¿ç”¨ '{source_return_col}' ä½œä¸ºæºã€‚")
        else:
            if 'forward_returns' in df_with_targets.columns:
                 source_return_col = 'forward_returns'
                 processing_mode = 'train' 
                 print(f"  > [å…¼å®¹æ¨¡å¼] åœ¨æµ‹è¯•æµç¨‹ä¸­å‘ç° '{source_return_col}'ï¼Œå°†æŒ‰è®­ç»ƒé›†é€»è¾‘è®¡ç®—ã€‚")
            else:
                print(f"  > âŒ è‡´å‘½é”™è¯¯: æµ‹è¯•æ¨¡å¼ä¸‹æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„æ”¶ç›Šåˆ—ã€‚")
                return df
            
    for horizon in TARGET_HORIZONS:
        shifted_returns = []

        # [å…³é”®ä¿®å¤] æ ¹æ®æ”¶ç›Šåˆ—ç±»å‹è°ƒæ•´èµ·ç‚¹ï¼š
        #  - forward_returns å·²ä¸å½“å‰è¡Œå¯¹é½ï¼Œåº”ä» i=0 å¼€å§‹ç´¯åŠ ï¼›
        #  - lagged_forward_returns è½åä¸€å¤©ï¼Œéœ€è¦ä» i=1 å¼€å§‹æ¨è¿›æ‰èƒ½å›åˆ°å½“å‰è¡Œçš„æœªæ¥æ”¶ç›Šã€‚
        if source_return_col == 'lagged_forward_returns':
            shift_start = 1
        else:
            shift_start = 0

        for i in range(shift_start, shift_start + horizon):
            shifted_returns.append(df_with_targets[source_return_col].shift(-i))

        multi_day_matrix = pd.concat(shifted_returns, axis=1)

        # å½“æœªæ¥å¤©æ•°ä¸è¶³ horizon æ—¶å¼ºåˆ¶è¾“å‡º NaNï¼Œé¿å…è¢«æˆªæ–­çš„æ”¶ç›Šæ··å…¥æ ‡ç­¾
        multi_day_return = multi_day_matrix.sum(axis=1, min_count=horizon)

        # 1. åˆ›å»ºåŸå§‹æ”¶ç›Šåˆ—(resp)
        resp_col_name = f'resp_{horizon}d'
        df_with_targets[resp_col_name] = multi_day_return
        
        # 2. åˆ›å»ºç¡¬æ ‡ç­¾åˆ†ç±»ç›®æ ‡ (action)
        action_col_name = f'action_{horizon}d'
        conditions = [
            df_with_targets[resp_col_name] > 0,
            df_with_targets[resp_col_name] <= 0
        ]
        choices = [1, 0]

        # --- [!!! å…³é”®ä¿®å¤ !!!] ---
        # è¡¥ä¸Šè¿™ä¸€è¡Œï¼Œæ¥çœŸæ­£åœ°åˆ›å»º action_*d åˆ—ï¼Œç”¨äºæœ€ç»ˆè¯„ä¼°
        df_with_targets[action_col_name] = np.select(conditions, choices, default=np.nan)
        # --- [ä¿®å¤ç»“æŸ] ---
        
        # 3. åˆ›å»ºåŠ¨æ€å¹³æ»‘æ ‡ç­¾ (dls_target)
        ALPHA = 200 
        dls_target_col_name = f'dls_target_{horizon}d'
        df_with_targets[dls_target_col_name] = 1 / (1 + np.exp(-ALPHA * df_with_targets[resp_col_name]))
        
    print(f"  > âœ… å¤šç›®æ ‡åˆ—ç”Ÿæˆå®Œæˆ (å·²åŒ…å« action_*d å’Œ dls_target_*d)ã€‚")
    return df_with_targets

# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="ç»ˆæç‰¹å¾å·¥ç¨‹è„šæœ¬ V4.0 (ç²¾è‹±æ¨¡å¼ç‰ˆ)")
    parser.add_argument('--train_input', type=str, required=True, help="è¾“å…¥åŸå§‹è®­ç»ƒCSV (e.g., train.csv)")
    parser.add_argument('--test_input', type=str, required=True, help="è¾“å…¥åŸå§‹æµ‹è¯•CSV (e.g., test.csv)")
    parser.add_argument('--train_output', type=str, required=True, help="è¾“å‡ºæœ€ç»ˆè®­ç»ƒç‰¹å¾CSV")
    parser.add_argument('--test_output', type=str, required=True, help="è¾“å‡ºæœ€ç»ˆæµ‹è¯•ç‰¹å¾CSV")
    # --- æ–°å¢æ¨¡å¼å¼€å…³ ---
    parser.add_argument('--mode', type=str, default='full', choices=['full', 'lite'], help="é€‰æ‹©æ¨¡å¼: 'full' (å…¨éƒ¨æ½œåŠ›è‚¡) æˆ– 'lite' (Top 12ç²¾è‹±)")
    args = parser.parse_args()

    start_time = time.time()
    print("="*50 + f"\nç»ˆæç‰¹å¾å·¥ç¨‹æµç¨‹å¯åŠ¨ (æ¨¡å¼: {args.mode.upper()})\n" + "="*50)

    # 1. åŠ è½½æ•°æ®
    print("\n1. æ­£åœ¨åŠ è½½æºæ•°æ®...")
    try:
        train_df_raw = pd.read_csv(args.train_input)
        test_df_raw = pd.read_csv(args.test_input)
        print(f"  > {args.train_input}: {len(train_df_raw)} è¡Œ | {args.test_input}: {len(test_df_raw)} è¡Œ")
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°è¾“å…¥æ–‡ä»¶ {e}ã€‚"); exit()
    
    # 2. å®šä¹‰ç‰¹å¾å·¥ç¨‹å‚æ•°
    # --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘æ ¹æ®æ¨¡å¼é€‰æ‹©ä¸åŒçš„â€œç§å­â€ç‰¹å¾ ---
    if args.mode == 'lite':
        print("\nğŸ”¥ å·²å¯åŠ¨ [ç²¾è‹±æ¨¡å¼]: åªä¸ºTop 12åŸå§‹ç‰¹å¾ç”Ÿæˆè¡ç”Ÿç‰¹å¾ã€‚")
        # !!! è¯·å°†ä¸‹é¢åˆ—è¡¨æ›¿æ¢ä¸ºæ‚¨åœ¨ç¬¬ä¸€æ­¥ä¸­æ‰¾åˆ°çš„çœŸå®Top 12ç‰¹å¾ !!!
        BASE_FEATURES_FOR_ENGINEERING = [
            'M4', 'P4', 'P3', 'V3', 'E19', 'P7', 'S2', 'P13', 'M3', 'P12', 'S5', 'S6' 
        ]
    else: # full æ¨¡å¼
        print("\nğŸ’£ å·²å¯åŠ¨ [å¸¸è§„æ¨¡å¼]: ä¸ºæ‰€æœ‰æ½œåŠ›è‚¡ç”Ÿæˆè¡ç”Ÿç‰¹å¾ã€‚")
        BASE_FEATURES_FOR_ENGINEERING = [
            'E19', 'P12', 'P4', 'P3', 'P7', 'P13', 'M15', 'M4', 'V3', 'S2', 'S5', 
            'M3', 'S6', 'M9', 'P1', 'I6', 'P6', 'V13', 'P5', 'M12', 'V2', 'I3', 
            'M7', 'I8', 'V5', 'E14', 'E4', 'I4', 'E11', 'I7'
        ]

    MAX_LOOKBACK_WINDOW = 60
    resp_cols = [f'resp_{h}d' for h in [1, 3, 5]]

    target_cols_to_check = resp_cols

    # --- åç»­æ‰€æœ‰æµç¨‹ (æµç¨‹A, æµç¨‹B) ä¿æŒå®Œå…¨ä¸å˜ ---
    # ... (ä» â€œæµç¨‹A: å¤„ç†è®­ç»ƒé›†â€ å¼€å§‹çš„å‰©ä½™ä»£ç ï¼Œæ— éœ€ä»»ä½•æ”¹åŠ¨) ...
    # --- æµç¨‹A: å¤„ç†è®­ç»ƒé›† (å®Œå…¨ç‹¬ç«‹) ---
    print("\n" + "-"*20 + " æ­£åœ¨å¤„ç†ã€è®­ç»ƒé›†ã€‘ " + "-"*20)
    print("2A. æ­£åœ¨ä¸ºè®­ç»ƒé›†ç”Ÿæˆæ‰‹å·¥ç‰¹å¾...")
    train_df_eng = create_manual_features(train_df_raw, BASE_FEATURES_FOR_ENGINEERING)
    print("3A. æ­£åœ¨ä¸ºè®­ç»ƒé›†æ·»åŠ ç›®æ ‡åˆ—...")
    train_df_targets = create_multi_horizon_targets(train_df_eng, processing_mode='train')
    print("4A. æ­£åœ¨ä¸ºè®­ç»ƒé›†æ·»åŠ æ ·æœ¬æƒé‡...")
    train_df_weights = create_safe_sample_weights(train_df_targets, resp_cols)
    print("5A. æ­£åœ¨å¯¹è®­ç»ƒé›†è¿›è¡Œæœ€ç»ˆå®‰å…¨å¤„ç†...")
    original_rows = len(train_df_weights)
    train_df_final = train_df_weights.dropna(subset=target_cols_to_check).copy()
    print(f"  > å·²ä»è®­ç»ƒé›†ç§»é™¤ {original_rows - len(train_df_final)} è¡Œã€‚")

    # --- æµç¨‹B: å¤„ç†æµ‹è¯•é›† (ä½¿ç”¨è®­ç»ƒé›†çš„â€œå°¾å·´â€ä½œä¸ºå†å²) ---
    print("\n" + "-"*20 + " æ­£åœ¨å¤„ç†ã€æµ‹è¯•é›†ã€‘ " + "-"*20)
    print(f"2B. æ­£åœ¨å‡†å¤‡ {MAX_LOOKBACK_WINDOW} å¤©çš„å†å²æ•°æ®ä½œä¸ºä¸Šä¸‹æ–‡...")
    train_lookback = train_df_raw.tail(MAX_LOOKBACK_WINDOW)
    test_with_lookback = pd.concat([train_lookback, test_df_raw], ignore_index=True)
    print("3B. æ­£åœ¨ä¸ºæµ‹è¯•é›†ç”Ÿæˆæ‰‹å·¥ç‰¹å¾ (å¸¦å†å²ä¸Šä¸‹æ–‡)...")
    test_df_eng_full = create_manual_features(test_with_lookback, BASE_FEATURES_FOR_ENGINEERING)
    print("4B. æ­£åœ¨ä»ç”Ÿæˆç»“æœä¸­å‰¥ç¦»å‡ºçº¯å‡€çš„æµ‹è¯•é›†ç‰¹å¾...")
    test_df_final = test_df_eng_full.tail(len(test_df_raw)).copy()
    print("5B. æ­£åœ¨ä¸ºã€æµ‹è¯•é›†ã€‘æ·»åŠ ç›®æ ‡åˆ—ï¼ˆç”¨äºè¯„ä¼°ï¼‰...")
    test_df_targets = create_multi_horizon_targets(test_df_final, processing_mode='test')
    print("6B. æ­£åœ¨ä¸ºã€æµ‹è¯•é›†ã€‘æ·»åŠ æ ·æœ¬æƒé‡ï¼ˆç”¨äºæµç¨‹ç»Ÿä¸€ï¼‰...")
    test_df_final_with_weights = create_safe_sample_weights(test_df_targets, resp_cols)
    print("7B. æ­£åœ¨å¯¹æµ‹è¯•é›†è¿›è¡Œæœ€ç»ˆå®‰å…¨å¤„ç†...")
    original_rows_test = len(test_df_final_with_weights)
    test_df_final_clean = test_df_final_with_weights.dropna(subset=resp_cols).copy()
    print(f"  > å·²ä»æµ‹è¯•é›†ç§»é™¤ {original_rows_test - len(test_df_final_clean)} è¡Œã€‚")

    # 6. ä¿å­˜æœ€ç»ˆæˆæœ
    print(f"\n6. æ­£åœ¨ä¿å­˜æœ€ç»ˆæˆæœ...")
    train_df_final.to_csv(args.train_output, index=False)
    test_df_final_clean.to_csv(args.test_output, index=False) 
    print(f"  > è®­ç»ƒé›† -> '{args.train_output}'")
    print(f"  > æµ‹è¯•é›† -> '{args.test_output}'")
    
    print("\n" + "="*50 + "\nâœ… ç»ˆæç‰¹å¾å·¥ç¨‹æµç¨‹èƒœåˆ©å®Œæˆï¼\n" + f"æ€»è€—æ—¶: {time.time() - start_time:.2f} ç§’ã€‚\n" + "="*50)
