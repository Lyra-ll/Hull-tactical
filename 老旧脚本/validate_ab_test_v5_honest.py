import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import roc_auc_score
import warnings
import json
import os

# =================================================================
# validate_ab_test_v5_honest.py (è¯šå®è¯„ä¼°ç‰ˆ)
# ç›®çš„: ä½¿ç”¨ä¸ä¸»æ¨¡å‹å®Œå…¨ä¸€è‡´çš„ã€æœ€å¯é çš„é¢„å¤„ç†æ–¹æ³•ï¼Œ
#       è¿›è¡Œä¸€æ¬¡ç»å¯¹â€œè¯šå®â€çš„ç‰¹å¾é›†A/Bæµ‹è¯•ã€‚
# =================================================================

warnings.filterwarnings('ignore')

# ================= 1. å…¨å±€é…ç½® (ä¿æŒä¸å˜) =================
RAW_DATA_FILE = 'train_v3_featured_raw.csv'
AE_FEATURES_FILE = 'train_v11_autotune_clf_ae_features.csv' 
PARAMS_FILE = 'best_params_v4_weighted_clf.json'
N_SPLITS = 5; PURGE_SIZE = 1; EMBARGO_SIZE = 40 
AI_PREFIX = 'AE_'; HANDMADE_SUFFIXES = ['_rol_', '_diff', '_rank']

# ================= 2. æ–°å¢ï¼šæˆ‘ä»¬æœ€å¯é çš„é¢„å¤„ç†å‡½æ•° =================
### æ–°å¢ ###
def preprocess_data(X_train, X_val):
    """
    ä½¿ç”¨ç»Ÿä¸€çš„ã€æ— æ•°æ®æ³„éœ²çš„é€»è¾‘æ¥å¤„ç†è®­ç»ƒé›†å’ŒéªŒè¯é›†ã€‚
    """
    X_train_filled = X_train.ffill(limit=3)
    median_filler = X_train_filled.median()
    X_train_filled.fillna(median_filler, inplace=True)
    X_train_filled.fillna(0, inplace=True)
    
    X_val_filled = X_val.ffill(limit=3)
    X_val_filled.fillna(median_filler, inplace=True)
    X_val_filled.fillna(0, inplace=True)
    
    return X_train_filled, X_val_filled

# ================= 3. æ ¸å¿ƒéªŒè¯å‡½æ•° (å·²å‡çº§) =================
def run_validation(X, y, sample_weight, params, group_name):
    print(f"\n{'='*20} å¼€å§‹æµ‹è¯•: {group_name} {'='*20}")
    print(f"    åŒ…å« {X.shape[1]} ä¸ªç‰¹å¾ã€‚")
    
    tscv = TimeSeriesSplit(n_splits=N_SPLITS, gap=EMBARGO_SIZE)
    fold_scores = []
    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
        purged_train_idx = train_idx[:-PURGE_SIZE]
        X_train_raw, y_train = X.iloc[purged_train_idx], y.iloc[purged_train_idx]
        X_val_raw, y_val = X.iloc[val_idx], y.iloc[val_idx]
        sw_train = sample_weight.iloc[purged_train_idx]
        sw_val = sample_weight.iloc[val_idx]
        
        ### å‡çº§ ###
        # æ›¿æ¢æ‰æ—§çš„ã€æœ‰é—®é¢˜çš„ np.nan_to_num é€»è¾‘
        X_train_filled, X_val_filled = preprocess_data(X_train_raw, X_val_raw)
        
        model = lgb.LGBMClassifier(**params)
        model.fit(X_train_filled, y_train, 
                  sample_weight=sw_train,
                  eval_set=[(X_val_filled, y_val)],
                  eval_sample_weight=[sw_val],
                  eval_metric='auc',
                  callbacks=[lgb.early_stopping(100, verbose=False)])
                  
        preds = model.predict_proba(X_val_filled)[:, 1]
        score = roc_auc_score(y_val, preds)
        fold_scores.append(score)

    mean_score = np.mean(fold_scores); std_score = np.std(fold_scores)
    print(f"    âœ… {group_name} æµ‹è¯•å®Œæˆã€‚å¹³å‡AUC: {mean_score:.8f}")
    return mean_score, std_score

# ================= 4. æ•°æ®åŠ è½½ä¸ç‰¹å¾è¯†åˆ« (ä¿æŒä¸å˜) =================
print("--- æ­¥éª¤1ï¼šåŠ è½½æ•°æ®å¹¶å‡†å¤‡åˆ†ç±»ä»»åŠ¡ ---")
raw_df = pd.read_csv(RAW_DATA_FILE)
ae_features_df = pd.read_csv(AE_FEATURES_FILE)
with open(PARAMS_FILE, 'r') as f: best_params = json.load(f)
df = pd.merge(raw_df, ae_features_df, on='date_id', suffixes=('_orig', None))
modern_df = df[df['date_id'] > 1055].copy().reset_index(drop=True)
y = (modern_df['forward_returns'] > 0).astype(int)
sample_weight = modern_df['forward_returns'].abs()
all_feature_names = [c for c in modern_df.columns if c not in ['date_id', 'forward_returns', 'market_forward_excess_returns', 'risk_free_rate']]
ai_features = [f for f in all_feature_names if f.startswith(AI_PREFIX)]
handmade_features = [f for f in all_feature_names if any(suffix in f for suffix in HANDMADE_SUFFIXES)]
original_features = [f for f in all_feature_names if f not in ai_features and f not in handmade_features]

# ================= 5. A/Bæµ‹è¯•ä¸æœ€ç»ˆå®¡åˆ¤ (ä¿æŒä¸å˜) =================
print("\n--- æ­¥éª¤2ï¼šå¼€å§‹AIç‰¹å¾ä»·å€¼çš„â€œè¯šå®â€å¯¹å†³ ---")
final_params = best_params.copy()
final_params.update({'objective': 'binary', 'metric': 'auc', 'random_state': 42, 'n_jobs': -1})
results = {}
X_control = modern_df[original_features]
results['å¯¹ç…§ç»„ (çº¯åŸå§‹)'] = run_validation(X_control, y, sample_weight, final_params, "å¯¹ç…§ç»„: çº¯åŸå§‹ç‰¹å¾")
X_test = modern_df[original_features + ai_features]
results['å®éªŒç»„ (åŸå§‹+AI)'] = run_validation(X_test, y, sample_weight, final_params, "å®éªŒç»„: åŸå§‹+AIç‰¹å¾")
# ... (åç»­çš„æŠ¥å‘Šç”Ÿæˆéƒ¨åˆ†æ— éœ€ä¿®æ”¹) ...
print(f"\n\n{'='*25} AIç‰¹å¾A/Bæµ‹è¯•æœ€ç»ˆæˆ˜æŠ¥ {'='*25}")
print(f"{'æµ‹è¯•ç»„':<25} | {'ç‰¹å¾æ•°':<10} | {'å¹³å‡AUC':<20} | {'AUCæ ‡å‡†å·®':<20}")
print("-" * 85)
sorted_results = sorted(results.items(), key=lambda item: item[1][0] if not np.isnan(item[1][0]) else -np.inf, reverse=True)
feature_count_map = {'å¯¹ç…§ç»„ (çº¯åŸå§‹)': len(X_control.columns), 'å®éªŒç»„ (åŸå§‹+AI)': len(X_test.columns)}
for name, (auc, std) in sorted_results:
    feature_count = feature_count_map.get(name, 0)
    if not np.isnan(auc): print(f"{name:<25} | {feature_count:<10} | {auc:<20.8f} | {std:<20.8f}")
print("=" * 85)
control_auc = results.get('å¯¹ç…§ç»„ (çº¯åŸå§‹)', (np.nan,))[0]; test_auc = results.get('å®éªŒç»„ (åŸå§‹+AI)', (np.nan,))[0]
print("\n--- æœ€ç»ˆå®¡åˆ¤ ---")
if np.isnan(control_auc) or np.isnan(test_auc): print("å®¡åˆ¤æ— æ³•è¿›è¡Œï¼Œè‡³å°‘æœ‰ä¸€ç»„æˆç»©æ— æ•ˆã€‚")
elif test_auc > control_auc:
    improvement = ((test_auc - control_auc) / control_auc) * 100
    print(f"ğŸ† å®éªŒç»„èƒœå‡ºï¼"); print(f"   AIç‰¹å¾çš„åŠ å…¥ï¼Œä½¿å¾—æ¨¡å‹çš„å¹³å‡AUCä» {control_auc:.6f} æå‡è‡³ {test_auc:.6f}ã€‚")
    print(f"   è¿™æ˜¯ä¸€ä¸ª {improvement:+.2f}% çš„ç›¸å¯¹æ€§èƒ½æå‡ï¼")
else:
    print(f"âš–ï¸ å¯¹ç…§ç»„èƒœå‡ºæˆ–æŒå¹³ã€‚"); print(f"   å¯¹ç…§ç»„AUC: {control_auc:.6f} vs å®éªŒç»„AUC: {test_auc:.6f}")
print("=" * 85)