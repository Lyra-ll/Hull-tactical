# analyze_high_auc.py
# =================================================================
# â€œ0.544é«˜åˆ†ä¸“æ¡ˆç»„â€ä¸“ç”¨è°ƒæŸ¥è„šæœ¬ V1.0
# ç›®çš„: å¤ç°å¹¶åˆ†æå¯¼è‡´æ„å¤–é«˜åˆ†çš„ç‰¹å¾ã€‚
# =================================================================
import pandas as pd
import lightgbm as lgb
from sklearn.metrics import roc_auc_score
import warnings

# å¯¼å…¥æ‚¨é¡¹ç›®ä¸­çš„é…ç½®æ–‡ä»¶å’Œå·¥å…·åº“
import config
import utils 

warnings.filterwarnings('ignore', category=UserWarning)

print("="*80)
print("ğŸš€ [å¯åŠ¨] 0.544é«˜åˆ†ä¸“æ¡ˆè°ƒæŸ¥ç¨‹åº...")
print("="*80)

# --- 1. åŠ è½½æ•°æ® ---
print("\n--- æ­¥éª¤ 1: åŠ è½½æ•°æ® ---")
dev_df = utils.load_data(config.RAW_DATA_FILE, config.ANALYSIS_START_DATE_ID)
all_features = [c for c in dev_df.columns if c.startswith(('M', 'E', 'I', 'P', 'V', 'S', 'D')) or '_diff' in c or '_rol_' in c]
X = dev_df[all_features]
y = dev_df[config.TARGET_COLUMNS]
sample_weight = dev_df['sample_weight']
print("  > æ•°æ®åŠ è½½å®Œæˆã€‚")

# --- 2. ç²¾ç¡®æ¨¡æ‹Ÿ Fold 0 çš„ç¯å¢ƒ ---
print("\n--- æ­¥éª¤ 2: æ¨¡æ‹Ÿ Fold 0 çš„æ•°æ®åˆ‡åˆ† (æ— åŠ¨æ€ç­›é€‰) ---")
tscv = utils.get_cv_splitter(config.N_SPLITS, config.EMBARGO_SIZE, config.PURGE_SIZE)
train_idx, val_idx = next(tscv.split(X))
X_train, y_train, sw_train = X.iloc[train_idx], y.iloc[train_idx], sample_weight.iloc[train_idx]
X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]
print(f"  > Fold 0 æ•°æ®åˆ‡åˆ†å®Œæˆï¼Œè®­ç»ƒé›†å¤§å°: {len(X_train)}")

# --- 3. ä½¿ç”¨æœ‰æ¼æ´çš„ V5 é¢„å¤„ç†å™¨å¤„ç†æ•°æ® ---
print("\n--- æ­¥éª¤ 3: ä½¿ç”¨æ—§ç‰ˆ V5 é¢„å¤„ç†å™¨å¤„ç†æ•°æ® ---")
# åœ¨è¿™ä¸ªæµç¨‹ä¸­ï¼Œæˆ‘ä»¬çŸ¥é“é¢„å¤„ç†å™¨ä¼šäº§ç”Ÿå¤§é‡NaNï¼Œè¿™æ˜¯â€œæ¡ˆå‘ç°åœºâ€çš„ä¸€éƒ¨åˆ†
preprocessor_params = utils.get_preprocessor_params(X_train)
_, X_train_scaled = utils.apply_preprocessor(X_train, preprocessor_params)
_, X_val_scaled = utils.apply_preprocessor(X_val, preprocessor_params)
print(f"  > æ•°æ®å·²æŒ‰æ—§æ–¹æ³•å¤„ç†ï¼Œè®­ç»ƒé›†NaNæ•°é‡: {X_train_scaled.isnull().sum().sum()}")


# --- 4. è®­ç»ƒLGBMå¹¶å¤ç°åˆ†æ•° ---
print("\n--- æ­¥éª¤ 4: è®­ç»ƒLGBMæ¨¡å‹å¹¶éªŒè¯åˆ†æ•° ---")
target_col = config.PRIMARY_TARGET_COLUMN
model = lgb.LGBMClassifier(random_state=42, n_estimators=100)
model.fit(X_train_scaled, y_train[target_col], sample_weight=sw_train)
val_preds = model.predict_proba(X_val_scaled)[:, 1]
score = roc_auc_score(y_val[target_col], val_preds)

print("\n" + "*"*35)
print(f"ğŸ“Š å¤ç°çš„åˆ†æ•° AUC: {score:.8f}")
print("*"*35)
if abs(score - 0.5448) < 0.001:
    print("  > âœ… æˆåŠŸï¼æˆ‘ä»¬å·²ç¨³å®šå¤ç°äº†é«˜åˆ†ç°åœºï¼")
else:
    print("  > âš ï¸ è­¦å‘Š: å¤ç°çš„åˆ†æ•°ä¸ç›®æ ‡æœ‰åå·®ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒã€‚")


# --- 5. æ ¸å¿ƒè°ƒæŸ¥ï¼šæå–ç‰¹å¾é‡è¦æ€§ ---
print("\n--- æ­¥éª¤ 5: æå–å¹¶ä¿å­˜â€œåŠŸå‹‹ç‰¹å¾â€åˆ—è¡¨ ---")
feature_importances = pd.Series(model.feature_importances_, index=X_train_scaled.columns)
# è¿‡æ»¤æ‰é‡è¦æ€§ä¸º0çš„ç‰¹å¾ï¼ˆåŒ…å«é‚£äº›å…¨NaNçš„åˆ—ï¼‰
feature_importances = feature_importances[feature_importances > 0]
feature_importances = feature_importances.sort_values(ascending=False)

output_filename = 'feature_importance_0.544_model.csv'
feature_importances.to_csv(output_filename)

print(f"  > âœ… ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜è‡³ '{output_filename}'")
print("\n--- è¯·æŸ¥çœ‹è¯¥æ–‡ä»¶ï¼Œæ’åœ¨æœ€å‰é¢çš„å°±æ˜¯â€œåŠŸå‹‹ç‰¹å¾â€ ---")
print(feature_importances.head(20)) # æ‰“å°å‰20å

print("\n" + "="*80)
print("ğŸ [å®Œæˆ] è°ƒæŸ¥ç»“æŸã€‚")
print("="*80)