# rank_original_features.py
# =================================================================
# åŸå§‹ç‰¹å¾â€œç«åŠ›ä¾¦å¯Ÿâ€è„šæœ¬ V1.0
# ç›®çš„ï¼šåœ¨æœ€åŸå§‹çš„æ•°æ®ä¸Šï¼Œä¸ºæ‰€æœ‰åŸç”Ÿç‰¹å¾æä¾›ä¸€ä¸ªå¯é çš„é‡è¦æ€§æ’åã€‚
# =================================================================
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import TimeSeriesSplit
import argparse
import time

def rank_features(train_file, output_file, start_date_id=1055):
    """
    å¯¹åŸå§‹CSVæ–‡ä»¶ä¸­çš„åŸç”Ÿç‰¹å¾è¿›è¡Œç¨³å¥çš„æ’åºã€‚
    """
    print("="*50 + "\nğŸš€ å¯åŠ¨åŸå§‹ç‰¹å¾ç«åŠ›ä¾¦å¯Ÿ...\n" + "="*50)
    
    # 1. åŠ è½½å¹¶ç­›é€‰æ•°æ®
    print(f"\n1. æ­£åœ¨åŠ è½½åŸå§‹æ•°æ®: '{train_file}'...")
    try:
        df = pd.read_csv(train_file)
        df = df[df['date_id'] >= start_date_id].copy()
        print(f"  > å·²ç­›é€‰ date_id >= {start_date_id} çš„æ•°æ®ï¼Œå‰©ä½™ {len(df)} è¡Œã€‚")
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°è¾“å…¥æ–‡ä»¶ '{train_file}'ã€‚"); return

    # 2. åˆ›å»ºç®€å•çš„ç›®æ ‡å’Œç‰¹å¾é›†
    print("\n2. æ­£åœ¨å®šä¹‰ç›®æ ‡å’Œç‰¹å¾é›†...")
    # å®šä¹‰ç›®æ ‡ï¼šæœªæ¥æ”¶ç›Šæ˜¯æ­£æ˜¯è´Ÿ
    y = (df['forward_returns'] > 0).astype(int)
    
    # å®šä¹‰ç‰¹å¾ï¼šæ‰€æœ‰éIDã€éæœªæ¥ä¿¡æ¯çš„åˆ—
    feature_cols = [c for c in df.columns if c.startswith(('M', 'E', 'I', 'P', 'V', 'S', 'D'))]
    X = df[feature_cols]
    print(f"  > å·²å®šä¹‰ç›®æ ‡åˆ—å’Œ {len(feature_cols)} ä¸ªåŸç”Ÿç‰¹å¾ã€‚")

    # 3. ç¨³å¥çš„äº¤å‰éªŒè¯æ’åº
    print("\n3. æ­£åœ¨é€šè¿‡æ—¶é—´åºåˆ—äº¤å‰éªŒè¯è¿›è¡Œç‰¹å¾æ’åº...")
    tscv = TimeSeriesSplit(n_splits=5)
    all_importances = []
    
    # åˆå§‹åŒ–LGBMåˆ†ç±»å™¨
    model = lgb.LGBMClassifier(random_state=42, verbosity=-1)

    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
        print(f"  > æ­£åœ¨å¤„ç†ç¬¬ {fold + 1}/5 æŠ˜...")
        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]
        
        # ç®€å•çš„ã€æ— æœªæ¥ä¿¡æ¯æ³„éœ²çš„å¡«å……ï¼Œä»…ä¸ºè®©æ¨¡å‹èƒ½è¿è¡Œ
        X_train_filled = X_train.ffill().fillna(X_train.median()).fillna(0)
        
        model.fit(X_train_filled, y_train)
        
        # è®°å½•æœ¬æŠ˜çš„ç‰¹å¾é‡è¦æ€§
        fold_importance = pd.Series(model.feature_importances_, index=feature_cols)
        all_importances.append(fold_importance)

    # 4. è®¡ç®—å¹³å‡é‡è¦æ€§å¹¶ä¿å­˜
    print("\n4. æ­£åœ¨è®¡ç®—å¹³å‡é‡è¦æ€§å¹¶ä¿å­˜ç»“æœ...")
    # å°†æ‰€æœ‰æŠ˜çš„é‡è¦æ€§åˆå¹¶å¹¶è®¡ç®—å¹³å‡å€¼
    avg_importance = pd.concat(all_importances, axis=1).mean(axis=1)
    # æŒ‰é‡è¦æ€§ä»é«˜åˆ°ä½æ’åº
    final_ranking = avg_importance.sort_values(ascending=False)
    
    final_ranking.to_csv(output_file, header=False)
    print(f"\nâœ… ç«åŠ›ä¾¦å¯Ÿå®Œæˆï¼åŸå§‹ç‰¹å¾æ’åå·²ä¿å­˜è‡³: '{output_file}'")
    print("\n--- æ’åå‰15çš„åŸå§‹ç‰¹å¾ ---")
    print(final_ranking.head(15))
    print("="*50)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="åŸå§‹ç‰¹å¾é‡è¦æ€§æ’åºè„šæœ¬")
    parser.add_argument('--input', type=str, default='train.csv', help="è¾“å…¥çš„åŸå§‹è®­ç»ƒCSVæ–‡ä»¶")
    parser.add_argument('--output', type=str, default='original_feature_ranking.csv', help="è¾“å‡ºçš„ç‰¹å¾æ’åCSVæ–‡ä»¶")
    args = parser.parse_args()
    
    start_time = time.time()
    rank_features(args.input, args.output)
    print(f"æ€»è€—æ—¶: {time.time() - start_time:.2f} ç§’ã€‚")