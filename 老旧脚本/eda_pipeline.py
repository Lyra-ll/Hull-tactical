import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import random
import os

# --- é…ç½® ---
sns.set_style('whitegrid')
warnings.filterwarnings('ignore')
# ä¿®å¤ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
plt.rcParams['font.sans-serif'] = ['SimHei'] 
plt.rcParams['axes.unicode_minus'] = False

# --- åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹ ---
if not os.path.exists('eda_reports'):
    os.makedirs('eda_reports')

# --- å¤ç”¨å‰åºæ­¥éª¤çš„å‡½æ•° ---
def load_data(file_path):
    """åŠ è½½CSVæ–‡ä»¶ã€‚"""
    try:
        df = pd.read_csv(file_path, low_memory=False)
        print("âœ… æ­¥éª¤ 1: åŸå§‹æ•°æ®åŠ è½½æˆåŠŸï¼")
        return df
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæ— æ³•æ‰¾åˆ°æ–‡ä»¶ '{file_path}'ã€‚")
        return None

def preprocess_for_eda(df, start_coverage_threshold=0.80):
    """
    ä¸“ä¸ºEDAè®¾è®¡çš„é¢„å¤„ç†ï¼šåªæˆªå–ï¼Œä¸å¡«å……ã€‚
    """
    print(f"\n--- æ­¥éª¤ 2: å¼€å§‹ä¸ºEDAè¿›è¡Œæ•°æ®é¢„å¤„ç† (è¦†ç›–ç‡é˜ˆå€¼={start_coverage_threshold*100}%) ---")
    high_info_cols = [col for col in df.columns if not col.startswith('D') and col != 'date_id' and 'returns' not in col and 'rate' not in col]
    start_indices = [df[col].first_valid_index() for col in high_info_cols if df[col].first_valid_index() is not None]
    start_indices.sort()
    threshold_index = int(len(start_indices) * start_coverage_threshold)
    smart_start_index = start_indices[threshold_index]
    
    df_modern = df.loc[smart_start_index:].reset_index(drop=True)
    print(f"      å·²ä»ç´¢å¼• {smart_start_index} å¼€å§‹æˆªå– {len(df_modern)} è¡Œé«˜è´¨é‡æ•°æ®ã€‚")
    print("      ç­–ç•¥ï¼šä¸ºä¿è¯EDAçš„çœŸå®æ€§ï¼Œä¸è¿›è¡Œä»»ä½•å¡«å……ã€‚")
    return df_modern

# --- EDA æ ¸å¿ƒå‡½æ•° (åœ¨åŸå§‹æ•°æ®ä¸Šè¿è¡Œ) ---

def analyze_feature_distributions_to_csv(df, feature_cols):
    """è®¡ç®—ç‰¹å¾çš„æè¿°æ€§ç»Ÿè®¡ä¿¡æ¯å¹¶ä¿å­˜åˆ°CSVã€‚"""
    print("\n--- EDAä»»åŠ¡1: åˆ†æç‰¹å¾åˆ†å¸ƒ (è¾“å‡ºè‡³è¡¨æ ¼) ---")
    
    # .describe() ä¼šè‡ªåŠ¨å¿½ç•¥NaNå€¼è¿›è¡Œè®¡ç®—
    stats_df = df[feature_cols].describe().transpose()
    # .skew() å’Œ .kurtosis() åŒæ ·ä¼šè‡ªåŠ¨å¤„ç†NaN
    stats_df['skew'] = df[feature_cols].skew()
    stats_df['kurtosis'] = df[feature_cols].kurtosis()
    
    save_path = 'eda_reports/feature_distributions_honest.csv'
    stats_df.to_csv(save_path)
    print(f"      âœ… â€œè¯šå®â€çš„ç‰¹å¾åˆ†å¸ƒç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜è‡³: {save_path}")

def analyze_correlation_to_csv(df, feature_cols, target_col='market_forward_excess_returns'):
    """è®¡ç®—ç‰¹å¾ä¸ç›®æ ‡çš„ç›¸å…³æ€§å¹¶ä¿å­˜åˆ°CSVã€‚"""
    print("\n--- EDAä»»åŠ¡2: åˆ†æç‰¹å¾ä¸ç›®æ ‡çš„ç›¸å…³æ€§ (è¾“å‡ºè‡³è¡¨æ ¼) ---")
    
    # .corr() åœ¨è®¡ç®—æ—¶é»˜è®¤ä¼šå¿½ç•¥æ‰å­˜åœ¨NaNçš„è¡Œå¯¹
    correlations = df[feature_cols + [target_col]].corr()[target_col].drop(target_col)
    corr_df = correlations.to_frame(name='correlation').sort_values('correlation', ascending=False)

    save_path = 'eda_reports/feature_target_correlation_honest.csv'
    corr_df.to_csv(save_path)
    print(f"      âœ… â€œè¯šå®â€çš„ç‰¹å¾ç›¸å…³æ€§æŠ¥å‘Šå·²ä¿å­˜è‡³: {save_path}")

def plot_features_over_time(df, feature_cols, target_col='market_forward_excess_returns', n_samples=4):
    """ç»˜åˆ¶ç‰¹å¾å’Œç›®æ ‡éšæ—¶é—´å˜åŒ–çš„è¶‹åŠ¿å›¾ã€‚"""
    print("\n--- EDAä»»åŠ¡3: è§‚å¯Ÿç‰¹å¾å’Œç›®æ ‡çš„æ—¶åºå˜åŒ– (è¾“å‡ºè‡³å›¾ç‰‡) ---")
    
    sample_features = random.sample(feature_cols, min(n_samples, len(feature_cols)))
    
    fig, axes = plt.subplots(len(sample_features) + 1, 1, figsize=(15, 10), sharex=True)
    
    # cumsum() åœ¨è®¡ç®—å‰ä¼šè‡ªåŠ¨æŠŠNaNå½“ä½œ0å¤„ç†ï¼Œè¿™é‡Œç»“æœå½±å“ä¸å¤§
    axes[0].plot(df['date_id'], df[target_col].cumsum())
    axes[0].set_title('ç›®æ ‡å˜é‡çš„ç´¯è®¡å€¼ (æ¨¡æ‹Ÿè‚¡å¸‚èµ°å‘)')
    axes[0].grid(True)

    for i, col in enumerate(sample_features, 1):
        # plot() ä¼šåœ¨æœ‰NaNçš„åœ°æ–¹è‡ªåŠ¨äº§ç”Ÿæ–­ç‚¹
        axes[i].plot(df['date_id'], df[col])
        axes[i].set_title(f'ç‰¹å¾ "{col}" éšæ—¶é—´çš„å˜åŒ–')
        axes[i].grid(True)
        
    plt.xlabel('Date ID (æ—¶é—´)')
    plt.tight_layout()
    save_path = 'eda_reports/features_over_time_honest.png'
    plt.savefig(save_path)
    plt.close()
    print(f"      âœ… â€œè¯šå®â€çš„æ—¶åºè¶‹åŠ¿å›¾å·²ä¿å­˜è‡³: {save_path}")

# --- æ‰§è¡Œå…¥å£ ---
if __name__ == "__main__":
    TRAIN_FILE_PATH = 'train.csv'
    
    raw_data = load_data(TRAIN_FILE_PATH)
    
    if raw_data is not None:
        # ä½¿ç”¨ä¸“ä¸ºEDAè®¾è®¡çš„ã€ä¸å¡«å……çš„é¢„å¤„ç†å‡½æ•°
        processed_data_for_eda = preprocess_for_eda(raw_data.copy(), start_coverage_threshold=0.80)
        
        TARGET_COL = 'market_forward_excess_returns'
        feature_cols = [col for col in processed_data_for_eda.columns if col not in ['date_id', 'forward_returns', 'risk_free_rate', TARGET_COL]]

        # æ‰§è¡Œæ‰€æœ‰â€œè¯šå®â€çš„EDAä»»åŠ¡
        analyze_feature_distributions_to_csv(processed_data_for_eda, feature_cols)
        analyze_correlation_to_csv(processed_data_for_eda, feature_cols, target_col=TARGET_COL)
        plot_features_over_time(processed_data_for_eda, feature_cols, target_col=TARGET_COL)
        
        print("\nğŸ‰ ä½œæˆ˜åœ°å›¾V6.0 - è¡ŒåŠ¨1.1ï¼šâ€œè¯šå®â€çš„æˆ˜åœºæµ‹ç»˜å®Œæˆï¼")
