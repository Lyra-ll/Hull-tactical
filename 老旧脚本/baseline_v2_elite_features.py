import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# --- å…¨å±€é…ç½® ---
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

def find_robust_start_date(df: pd.DataFrame, completeness_threshold: float = 0.75) -> int:
    # ... (æ­¤å‡½æ•°æ— å˜åŒ–)
    print(f"--- ä»»åŠ¡ 1: åŠ¨æ€è®¡ç®—åˆ†æèµ·ç‚¹ (ç‰¹å¾é½å…¨ç‡ > {completeness_threshold * 100}%) ---")
    feature_cols = [col for col in df.columns if col.startswith(('D', 'E', 'I', 'M', 'P', 'S', 'V'))]
    df['feature_completeness'] = df[feature_cols].notna().sum(axis=1) / len(feature_cols)
    robust_start_date = df.loc[df['feature_completeness'] >= completeness_threshold, 'date_id'].iloc[0]
    print(f"è®¡ç®—å®Œæˆï¼ç¨³å¥çš„åˆ†æèµ·ç‚¹ date_id ä¸º: {robust_start_date}")
    return robust_start_date

def select_top_features_by_gain(X_train, y_train, all_features, lgb_params, n_top_features=50):
    # ... (æ­¤å‡½æ•°æ— å˜åŒ–)
    print(f"\n--- [ç­›é€‰é˜¶æ®µ] æ­£åœ¨è®­ç»ƒä¾¦å¯Ÿæ¨¡å‹ä»¥è¿›è¡Œç‰¹å¾æ’åº... ---")
    recon_model = lgb.LGBMRegressor(**lgb_params)
    recon_model.fit(X_train, y_train, eval_metric='rmse')
    feature_importance_df = pd.DataFrame({
        'feature': all_features,
        'gain': recon_model.feature_importances_
    }).sort_values('gain', ascending=False)
    elite_features = feature_importance_df.head(n_top_features)['feature'].tolist()
    print(f"--- [ç­›é€‰é˜¶æ®µ] ç‰¹å¾ç­›é€‰å®Œæˆï¼å·²é€‰å‡ºTop {n_top_features} ç²¾è‹±ç‰¹å¾ã€‚---")
    print("æ’åå‰5çš„ç‰¹å¾æ˜¯:", elite_features[:5])
    return elite_features

def train_and_evaluate(df: pd.DataFrame, start_date_id: int, num_elite_features: int = 50, imputation_strategy: str = 'none'):
    """
    ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°æµç¨‹ï¼Œè¿”å›æœ€ç»ˆçš„RMSEåˆ†æ•°ã€‚
    """
    print(f"\n{'='*70}")
    print(f"  å¼€å§‹æ–°æˆ˜å½¹ | éƒ¨é˜Ÿè§„æ¨¡: Top {num_elite_features} | ç¼ºå¤±å€¼ç­–ç•¥: {imputation_strategy.upper()}")
    print(f"{'='*70}")
    
    train_filtered = df[df['date_id'] > start_date_id].copy()

    # æˆ‘ä»¬å·²ç»ç¡®å®š'none'æ˜¯æœ€ä½³ç­–ç•¥ï¼Œæ‰€ä»¥åªä¿ç•™è¿™ä¸€ä¸ªåˆ†æ”¯
    print("\n[æ•°æ®å¤„ç†] é‡‡ç”¨ 'none' (ä¸å¡«å……) ç­–ç•¥ã€‚")
    train_processed = train_filtered
    
    all_features = [col for col in train_processed.columns if col.startswith(('D', 'E', 'I', 'M', 'P', 'S', 'V'))]
    target = 'forward_returns'
    
    X = train_processed[all_features]
    y = train_processed[target].fillna(0)

    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)

    lgb_params = {
        'objective': 'regression_l1', 'metric': 'rmse', 'n_estimators': 5000,
        'learning_rate': 0.01, 'feature_fraction': 0.8, 'bagging_fraction': 0.8,
        'bagging_freq': 1, 'verbose': -1, 'n_jobs': -1, 'seed': 42, 
        'importance_type': 'gain', 'num_leaves': 31
    }

    elite_features = select_top_features_by_gain(X_train, y_train, all_features, lgb_params, n_top_features=num_elite_features)

    print(f"\n--- [æ€»æ”»é˜¶æ®µ] æ­£åœ¨ä½¿ç”¨ {len(elite_features)} ä¸ªç²¾è‹±ç‰¹å¾è®­ç»ƒæœ€ç»ˆæ¨¡å‹... ---")
    X_train_elite = X_train[elite_features]
    X_val_elite = X_val[elite_features]

    final_model = lgb.LGBMRegressor(**lgb_params)
    final_model.fit(X_train_elite, y_train,
                    eval_set=[(X_val_elite, y_val)],
                    eval_metric='rmse',
                    callbacks=[lgb.early_stopping(100, verbose=False)])

    preds = final_model.predict(X_val_elite)
    rmse = np.sqrt(mean_squared_error(y_val, preds))
    
    print(f"\n--- [æˆ˜å½¹ç»“æŸ] éƒ¨é˜Ÿè§„æ¨¡ Top {num_elite_features} çš„RMSEä¸º: {rmse} ---")
    
    # ä¸ºæ¯ä¸ªæˆ˜å½¹ä¿å­˜ç‹¬ç«‹çš„Gainå›¾
    lgb.plot_importance(final_model, max_num_features=num_elite_features, figsize=(10, 12), importance_type='gain')
    plt.title(f'Top {num_elite_features} ç²¾è‹±ç‰¹å¾é‡è¦æ€§ (Gain)')
    plt.tight_layout()
    image_filename = f'feature_importance_top_{num_elite_features}.png'
    plt.savefig(image_filename)
    plt.close()
    
    return rmse

# ==============================================================================
# ä¸»æ‰§è¡Œæµç¨‹ (å·²å‡çº§ä¸ºè‡ªåŠ¨åŒ–æˆ˜å½¹å¾ªç¯)
# ==============================================================================
if __name__ == '__main__':
    try:
        print("æ­£åœ¨åŠ è½½ train_v3_featured_raw.csv...")
        train_df = pd.read_csv('train_v3_featured_raw.csv')

        robust_start_date = find_robust_start_date(train_df)
        
        # --- æ ¸å¿ƒä¿®æ”¹ï¼šè‡ªåŠ¨åŒ–æˆ˜å½¹å¾ªç¯ ---
        battle_plans = [30, 50, 80] # å®šä¹‰æˆ‘ä»¬è¦æµ‹è¯•çš„ä¸‰ç§éƒ¨é˜Ÿè§„æ¨¡
        results = {}

        for n_features in battle_plans:
            rmse_result = train_and_evaluate(
                train_df, 
                start_date_id=robust_start_date, 
                num_elite_features=n_features,
                imputation_strategy='none' # é”å®šæœ€ä¼˜ç­–ç•¥
            )
            results[f'Top_{n_features}_Features'] = rmse_result

        # --- æœ€ç»ˆæˆ˜æŠ¥æ±‡æ€» ---
        print("\n" + "="*70)
        print("!!!               æœ€ç»ˆæˆ˜æŠ¥æ±‡æ€»               !!!")
        print("="*70)
        
        best_plan = min(results, key=results.get)
        
        for plan, score in results.items():
            print(f">>> éƒ¨é˜Ÿè§„æ¨¡: {plan.replace('_', ' ')} | æœ€ç»ˆRMSE: {score}")
            
        print("\n--- [æœ€ç»ˆè£å†³] ---")
        print(f"ğŸ† æœ€ä¼˜éƒ¨é˜Ÿè§„æ¨¡ä¸º: {best_plan.replace('_', ' ')}ï¼Œå…¶RMSEæœ€ä½ï¼ğŸ†")
        print("="*70)

    except FileNotFoundError:
        print("\né”™è¯¯: train_v3_featured_raw.csv æ–‡ä»¶æœªæ‰¾åˆ°ã€‚è¯·ç¡®ä¿æ‚¨å·²æˆåŠŸè¿è¡Œä¿®æ”¹åçš„ create_features.pyã€‚")
    except Exception as e:
        print(f"\næ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")