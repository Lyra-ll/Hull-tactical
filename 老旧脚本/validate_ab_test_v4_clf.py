# =================================================================
# validate_ab_test_v4_clf.py (Feature Set A/B Test Platform - Classification)
# ç›®çš„: æä¾›ä¸€ä¸ªæ ‡å‡†åŒ–çš„ç‰¹å¾é›†A/Bæµ‹è¯•å¹³å°ï¼Œä¸“é—¨ç”¨äºâ€œåŠ æƒåˆ†ç±»â€ä»»åŠ¡ï¼Œ
#       ç§‘å­¦åœ°ã€å†³å®šæ€§åœ°è¯„ä¼°ä¸€ç»„æ–°ç‰¹å¾ï¼ˆå¦‚AIç‰¹å¾ï¼‰çš„çœŸå®è¾¹é™…è´¡çŒ®ã€‚
# =================================================================
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import roc_auc_score # <--- æ ¸å¿ƒå‡çº§ #1: è¯„ä¼°æŒ‡æ ‡æ”¹ä¸ºAUC
import warnings
import json
import os

warnings.filterwarnings('ignore')

# ================= 1. å…¨å±€é…ç½® =================
# --- æ–‡ä»¶è·¯å¾„ ---
RAW_DATA_FILE = 'train_v3_featured_raw.csv'
# ç¡®ä¿è¿™é‡Œä½¿ç”¨çš„æ˜¯æ‚¨æœ€æ–°çš„ã€ç»è¿‡å› æœéªŒè¯ç”Ÿæˆçš„AIç‰¹å¾æ–‡ä»¶
AE_FEATURES_FILE = 'train_v11_autotune_clf_ae_features.csv' 
# <--- æ ¸å¿ƒå‡çº§ #2: åŠ è½½ä¸ºâ€œåŠ æƒåˆ†ç±»â€æ¨¡å¼æ‰¾åˆ°çš„ä¸“å±é»„é‡‘å‚æ•°
PARAMS_FILE = 'best_params_v4_weighted_clf.json'

# --- éªŒè¯ç­–ç•¥ ---
N_SPLITS = 5
PURGE_SIZE = 1
EMBARGO_SIZE = 40 

# --- ç‰¹å¾â€œå…µç§â€è¯†åˆ«æŒ‡çº¹ ---
# æˆ‘ä»¬æš‚æ—¶ä¸è¯„ä¼°æ‰‹å·¥ç‰¹å¾ï¼Œæ‰€ä»¥è¿™ä¸ªåˆ—è¡¨å¯ä»¥ä¸ºç©ºï¼Œä½†ä¿ç•™æ¡†æ¶
HANDMADE_SUFFIXES = ['_rol_', '_diff', '_rank']
AI_PREFIX = 'AE_'

# ================= 2. æ ¸å¿ƒéªŒè¯å‡½æ•° (å·²ä¸ºåˆ†ç±»ä»»åŠ¡å…¨é¢å‡çº§) =================
def run_validation(X, y, sample_weight, params, group_name):
    """
    å¯¹ç»™å®šçš„ç‰¹å¾é›†Xå’Œç›®æ ‡yï¼Œåœ¨â€œåŠ æƒåˆ†ç±»â€æ¨¡å¼ä¸‹ï¼Œ
    æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„å‡€åŒ–ç¦è¿æ—¶åºäº¤å‰éªŒè¯ã€‚
    è¿”å›å¹³å‡AUCå’ŒAUCçš„æ ‡å‡†å·®ã€‚
    """
    if X.shape[1] == 0:
        print(f"\n{'='*20} è·³è¿‡æµ‹è¯•: {group_name} (æ— ç‰¹å¾) {'='*20}")
        return np.nan, np.nan
        
    print(f"\n{'='*20} å¼€å§‹æµ‹è¯•: {group_name} {'='*20}")
    print(f"    åŒ…å« {X.shape[1]} ä¸ªç‰¹å¾ã€‚")
    
    tscv = TimeSeriesSplit(n_splits=N_SPLITS, gap=EMBARGO_SIZE)
    fold_scores = []
    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
        purged_train_idx = train_idx[:-PURGE_SIZE]
        X_train_raw, y_train = X.iloc[purged_train_idx], y.iloc[purged_train_idx]
        X_val_raw, y_val = X.iloc[val_idx], y.iloc[val_idx]
        
        # æå–å¯¹åº”çš„æ ·æœ¬æƒé‡
        sw_train = sample_weight.iloc[purged_train_idx]
        sw_val = sample_weight.iloc[val_idx]
        
        mean = np.nanmean(X_train_raw.values, axis=0)
        std = np.nanstd(X_train_raw.values, axis=0)
        std[std == 0] = 1.0
        
        X_train_scaled = np.nan_to_num((X_train_raw.values - mean) / std, nan=0.0)
        X_val_scaled = np.nan_to_num((X_val_raw.values - mean) / std, nan=0.0)
        
        # <--- æ ¸å¿ƒå‡çº§ #3: ä½¿ç”¨LGBMClassifier
        model = lgb.LGBMClassifier(**params)
        
        # <--- æ ¸å¿ƒå‡çº§ #4: åœ¨fitæ–¹æ³•ä¸­ä¼ å…¥æ ·æœ¬æƒé‡
        model.fit(X_train_scaled, y_train, 
                  sample_weight=sw_train,
                  eval_set=[(X_val_scaled, y_val)],
                  eval_sample_weight=[sw_val],
                  eval_metric='auc',
                  callbacks=[lgb.early_stopping(100, verbose=False)])
                  
        # <--- æ ¸å¿ƒå‡çº§ #5: é¢„æµ‹æ¦‚ç‡å¹¶è®¡ç®—AUC
        preds = model.predict_proba(X_val_scaled)[:, 1]
        score = roc_auc_score(y_val, preds)
        fold_scores.append(score)

    mean_score = np.mean(fold_scores)
    std_score = np.std(fold_scores)
    print(f"    âœ… {group_name} æµ‹è¯•å®Œæˆã€‚å¹³å‡AUC: {mean_score:.8f}")
    return mean_score, std_score

# ================= 3. æ•°æ®åŠ è½½ä¸ç‰¹å¾â€œå…µç§â€è¯†åˆ« =================
print("--- æ­¥éª¤1ï¼šåŠ è½½æ•°æ®å¹¶å‡†å¤‡åˆ†ç±»ä»»åŠ¡ ---")
raw_df = pd.read_csv(RAW_DATA_FILE)
ae_features_df = pd.read_csv(AE_FEATURES_FILE)
with open(PARAMS_FILE, 'r') as f: best_params = json.load(f)
df = pd.merge(raw_df, ae_features_df, on='date_id', suffixes=('_orig', None))
if 'forward_returns_orig' in df.columns: df.drop(columns=['forward_returns_orig'], inplace=True)
modern_df = df[df['date_id'] > 1055].copy().reset_index(drop=True)

# <--- æ ¸å¿ƒå‡çº§ #6: å‡†å¤‡åˆ†ç±»ç›®æ ‡yå’Œæ ·æœ¬æƒé‡sample_weight
y = (modern_df['forward_returns'] > 0).astype(int)
sample_weight = modern_df['forward_returns'].abs()
print("    -> åˆ†ç±»ç›®æ ‡(y)å’Œæ ·æœ¬æƒé‡(sample_weight)å‡†å¤‡å®Œæ¯•ã€‚")

# è‡ªåŠ¨åŒ–è¯†åˆ«ä¸‰å¤§å…µç§
all_feature_names = [c for c in modern_df.columns if c not in ['date_id', 'forward_returns', 'market_forward_excess_returns', 'risk_free_rate']]
ai_features = [f for f in all_feature_names if f.startswith(AI_PREFIX)]
handmade_features = [f for f in all_feature_names if any(suffix in f for suffix in HANDMADE_SUFFIXES)]
original_features = [f for f in all_feature_names if f not in ai_features and f not in handmade_features]

print(f"å…µç§è¯†åˆ«å®Œæ¯•ï¼š{len(original_features)}ä¸ªåŸå§‹ç‰¹å¾, {len(ai_features)}ä¸ªAIç‰¹å¾, {len(handmade_features)}ä¸ªæ‰‹å·¥ç‰¹å¾ã€‚")

# ================= 4. å®šä¹‰å¯¹ç…§ç»„ä¸å®éªŒç»„ï¼Œå¼€å§‹A/Bæµ‹è¯• =================
print("\n--- æ­¥éª¤2ï¼šå¼€å§‹AIç‰¹å¾ä»·å€¼çš„ç»ˆæå¯¹å†³ ---")
final_params = best_params.copy()
# ç¡®ä¿æ¨¡å‹é…ç½®ä¸åˆ†ç±»ä»»åŠ¡åŒ¹é…
final_params.update({'objective': 'binary', 'metric': 'auc', 'random_state': 42, 'n_jobs': -1})

results = {}
# å¯¹ç…§ç»„: çº¯åŸå§‹éƒ¨é˜Ÿ
X_control = modern_df[original_features]
results['å¯¹ç…§ç»„ (çº¯åŸå§‹)'] = run_validation(X_control, y, sample_weight, final_params, "å¯¹ç…§ç»„: çº¯åŸå§‹ç‰¹å¾")

# å®éªŒç»„: æ··åˆéƒ¨é˜Ÿ (åŸå§‹ + AI)
X_test = modern_df[original_features + ai_features]
results['å®éªŒç»„ (åŸå§‹+AI)'] = run_validation(X_test, y, sample_weight, final_params, "å®éªŒç»„: åŸå§‹+AIç‰¹å¾")

# ================= 5. ç”Ÿæˆæœ€ç»ˆâ€œæˆ˜åŠ›æ’è¡Œæ¦œâ€ä¸å®¡åˆ¤ =================
print(f"\n\n{'='*25} AIç‰¹å¾A/Bæµ‹è¯•æœ€ç»ˆæˆ˜æŠ¥ {'='*25}")
print(f"{'æµ‹è¯•ç»„':<25} | {'ç‰¹å¾æ•°':<10} | {'å¹³å‡AUC':<20} | {'AUCæ ‡å‡†å·®':<20}")
print("-" * 85)

# å¯¹ç»“æœæŒ‰AUCé™åºæ’åº
sorted_results = sorted(results.items(), key=lambda item: item[1][0] if not np.isnan(item[1][0]) else -np.inf, reverse=True)

feature_count_map = {
    'å¯¹ç…§ç»„ (çº¯åŸå§‹)': len(X_control.columns),
    'å®éªŒç»„ (åŸå§‹+AI)': len(X_test.columns),
}
    
for name, (auc, std) in sorted_results:
    feature_count = feature_count_map.get(name, 0)
    
    if not np.isnan(auc):
        print(f"{name:<25} | {feature_count:<10} | {auc:<20.8f} | {std:<20.8f}")
    else:
        print(f"{name:<25} | {feature_count:<10} | {'N/A':<20} | {'N/A':<20}")
print("=" * 85)

# --- æœ€ç»ˆå®¡åˆ¤ ---
control_auc = results.get('å¯¹ç…§ç»„ (çº¯åŸå§‹)', (np.nan,))[0]
test_auc = results.get('å®éªŒç»„ (åŸå§‹+AI)', (np.nan,))[0]

print("\n--- æœ€ç»ˆå®¡åˆ¤ ---")
if np.isnan(control_auc) or np.isnan(test_auc):
    print("å®¡åˆ¤æ— æ³•è¿›è¡Œï¼Œè‡³å°‘æœ‰ä¸€ç»„æˆç»©æ— æ•ˆã€‚")
elif test_auc > control_auc:
    improvement = ((test_auc - control_auc) / control_auc) * 100
    print(f"ğŸ† å®éªŒç»„èƒœå‡ºï¼")
    print(f"   AIç‰¹å¾çš„åŠ å…¥ï¼Œä½¿å¾—æ¨¡å‹çš„å¹³å‡AUCä» {control_auc:.6f} æå‡è‡³ {test_auc:.6f}ã€‚")
    print(f"   è¿™æ˜¯ä¸€ä¸ª {improvement:+.2f}% çš„ç›¸å¯¹æ€§èƒ½æå‡ï¼")
    print(f"   ç»“è®ºï¼šAIç‰¹å¾åœ¨æ–°æˆ˜åœºè§„åˆ™ä¸‹ï¼Œæ˜¯æœ‰æ•ˆçš„â€œç©ºä¸­æ”¯æ´â€ï¼")
else:
    print(f"âš–ï¸ å¯¹ç…§ç»„èƒœå‡ºæˆ–æŒå¹³ã€‚")
    print(f"   AIç‰¹å¾çš„åŠ å…¥æœªèƒ½å¸¦æ¥æ˜ç¡®çš„AUCæå‡ã€‚")
    print(f"   å¯¹ç…§ç»„AUC: {control_auc:.6f} vs å®éªŒç»„AUC: {test_auc:.6f}")
    print(f"   ç»“è®ºï¼šAIç‰¹å¾çš„ä»·å€¼å°šæœªåœ¨æ­¤æ¬¡å¯¹å†³ä¸­å¾—åˆ°è¯æ˜ã€‚")
print("=" * 85)