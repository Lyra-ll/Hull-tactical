# main.py
# =================================================================
# è‡ªåŠ¨åŒ–æŒ‡æŒ¥ä¸­å¿ƒ V1.5.1 (æœ€ç»ˆä¿®æ­£å’Œå®Œæ•´ç‰ˆ)
# =================================================================
import argparse
import json
import time
import pandas as pd
import numpy as np
import lightgbm as lgb
import optuna
from sklearn.feature_selection import RFE
from sklearn.metrics import roc_auc_score
import torch
import copy

# å¯¼å…¥æˆ‘ä»¬çš„è‡ªå®šä¹‰æ¨¡å—
import config
import utils

# --- 1. ä½œæˆ˜æ¨¡å¼: ç‰¹å¾ç­›é€‰ (RFE) ---
def run_rfe(X, y):
    print("\n--- å¯åŠ¨ 'RFE' æ¨¡å¼: æ­£åœ¨è¿›è¡Œäº¤å‰éªŒè¯ç‰¹å¾æ’åº ---")
    estimator = lgb.LGBMClassifier(random_state=42, verbosity=-1)
    tscv = utils.get_cv_splitter(config.N_SPLITS, config.EMBARGO_SIZE, config.PURGE_SIZE)
    all_rankings = []
    
    for fold, (train_idx, _) in enumerate(tscv.split(X)):
        print(f"  > æ­£åœ¨ç¬¬ {fold + 1}/{config.N_SPLITS} æŠ˜ä¸Šè¿è¡ŒRFE...")
        purged_train_idx = train_idx[:-config.PURGE_SIZE] if config.PURGE_SIZE > 0 else train_idx
        X_train_rfe, y_train_rfe = X.iloc[purged_train_idx], y.iloc[purged_train_idx]
        X_train_rfe_filled = X_train_rfe.ffill(limit=3).fillna(X_train_rfe.median()).fillna(0)
        
        selector = RFE(estimator=estimator, n_features_to_select=config.N_FEATURES_TO_SELECT_RFE, step=0.1)
        selector.fit(X_train_rfe_filled, y_train_rfe)
        all_rankings.append(pd.Series(selector.ranking_, index=X.columns))

    avg_ranking = pd.concat(all_rankings, axis=1).mean(axis=1).sort_values()
    avg_ranking.to_csv(config.RANKING_FILE)
    print(f"\nâœ… RFEæ’åå®Œæˆï¼æ’åå·²ä¿å­˜è‡³ '{config.RANKING_FILE}'")

# --- 2. ä½œæˆ˜æ¨¡å¼: è¶…å‚æ•°æœç´¢ (Optuna) ---
def run_tuning(X, y, sample_weight, tune_target):
    
    tscv_fortuning = utils.get_cv_splitter(config.N_SPLITS, config.EMBARGO_SIZE, config.PURGE_SIZE)
    folds_to_use = list(tscv_fortuning.split(X))

    print(f"    > [ç»ˆæç¨³å¥æ€§] Optunaå°†ä½¿ç”¨å…¨éƒ¨ {len(folds_to_use)} æŠ˜C/Væ•°æ®è¿›è¡Œè¯„ä¼°...")

    def objective(trial):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        if tune_target == 'lgbm':
            # ... [åŠ è½½ae_paramsçš„é€»è¾‘æ— å˜åŒ–] ...
            try:
                with open(config.AE_PARAMS_FILE, 'r') as f:
                    ae_params = json.load(f)
            except FileNotFoundError:
                print(f"    > è­¦å‘Š: æœªæ‰¾åˆ°AEå‚æ•°æ–‡ä»¶ '{config.AE_PARAMS_FILE}'ã€‚å°†ä½¿ç”¨ä¸€å¥—é»˜è®¤çš„ç®€å•å‚æ•°ã€‚")
                ae_params = {'hidden_dim': 128, 'encoding_dim': 32, 'n_hidden_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'recon_weight': 0.5, 'bn':True}
            
            # [å‡çº§] ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰è®¨è®ºè¿‡çš„ã€æ›´ä¸“æ³¨å’ŒåŠ å¼ºæ­£åˆ™åŒ–çš„LGBMæœç´¢ç©ºé—´
            lgbm_params = {
                'objective': 'binary', 'metric': 'auc', 'random_state': 42, 'n_jobs': -1, 'verbosity': -1,
                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),
                'n_estimators': trial.suggest_int('n_estimators', 500, 4000),
                'num_leaves': trial.suggest_int('num_leaves', 10, 50),
                'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 50.0, log=True),
                'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 50.0, log=True),
                'colsample_bytree': 0.8,
                'subsample': 0.8,
                'min_child_samples': trial.suggest_int('min_child_samples', 20, 100),
            }
        
        elif tune_target == 'ae':
            # ... [åŠ è½½lgbm_paramsçš„é€»è¾‘æ— å˜åŒ–] ...
            with open(config.LGBM_PARAMS_FILE, 'r') as f: lgbm_params = json.load(f)
            lgbm_params.update({'objective': 'binary', 'metric': 'auc', 'random_state': 42, 'n_jobs': -1, 'verbosity': -1})
            
            # [å‡çº§] ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰è®¨è®ºè¿‡çš„ã€æ›´ä¸“æ³¨å’Œå¼ºåˆ¶å¼€å¯BNçš„AEæœç´¢ç©ºé—´
            ae_params = {
                'hidden_dim': trial.suggest_int('hidden_dim', 64, 256, step=32),
                'encoding_dim': trial.suggest_int('encoding_dim', 16, 64, step=8),
                'n_hidden_layers': trial.suggest_int('n_hidden_layers', 1, 4),
                'dropout_rate': 0.2,
                'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),
                'recon_weight': 0.5,
                'bn': True,
            }

        oof_predictions_trial = np.zeros(len(X))
        
        for fold, (train_idx, val_idx) in enumerate(folds_to_use):
            # ... [å¾ªç¯å†…éƒ¨çš„æ•°æ®å‡†å¤‡å’Œæ¨¡å‹è®­ç»ƒé€»è¾‘å®Œå…¨æ— å˜åŒ–] ...
            purged_train_idx = train_idx[:-config.PURGE_SIZE] if config.PURGE_SIZE > 0 else train_idx
            X_train, y_train = X.iloc[purged_train_idx], y.iloc[purged_train_idx]
            sw_train = sample_weight.iloc[purged_train_idx]
            X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]

            train_mask_np = X_train.isnull().values
            X_train_processed, X_val_processed = utils.preprocess_for_lgbm(X_train, X_val)

            X_train_np, y_train_np = X_train_processed.values, y_train.values
            sw_train_np, X_val_np = sw_train.values, X_val_processed.values
            y_val_np = y_val.values
            
            ae_models = utils.train_fold_ae(ae_params, X_train_np, y_train_np, sw_train_np, X_val_np, y_val_np, train_mask_np)
            
            if not ae_models:
                oof_predictions_trial[val_idx] = 0.5
                continue

            with torch.no_grad():
                # ... [ç‰¹å¾ç”Ÿæˆé€»è¾‘æ— å˜åŒ–] ...
                mean, std = np.mean(X_train_np, axis=0), np.std(X_train_np, axis=0)
                std[std==0] = 1.0
                train_scaled = torch.tensor((X_train_np - mean) / std, dtype=torch.float32).to(device)
                val_scaled = torch.tensor((X_val_np - mean) / std, dtype=torch.float32).to(device)
                all_train_ai, all_val_ai = [], []
                for ae_model in ae_models:
                    all_train_ai.append(ae_model.encoder(train_scaled).cpu().numpy())
                    all_val_ai.append(ae_model.encoder(val_scaled).cpu().numpy())
                X_train_ai, X_val_ai = np.mean(all_train_ai, axis=0), np.mean(all_val_ai, axis=0)

            # ... [ç‰¹å¾åˆå¹¶å’ŒLGBMè®­ç»ƒé€»è¾‘æ— å˜åŒ–] ...
            ai_feature_names = [f'{config.AI_PREFIX}{i}' for i in range(X_train_ai.shape[1])]
            X_train_ai_df = pd.DataFrame(X_train_ai, columns=ai_feature_names, index=X_train_processed.index)
            X_val_ai_df = pd.DataFrame(X_val_ai, columns=ai_feature_names, index=X_val_processed.index)
            X_train_combined = pd.concat([X_train_processed, X_train_ai_df], axis=1)
            X_val_combined = pd.concat([X_val_processed, X_val_ai_df], axis=1)
            X_train_final, X_val_final = utils.preprocess_for_lgbm(X_train_combined, X_val_combined)
            
            model_lgbm = lgb.LGBMClassifier(**lgbm_params)
            model_lgbm.fit(X_train_final, y_train, sample_weight=sw_train,
                           eval_set=[(X_val_final, y_val)],
                           callbacks=[lgb.early_stopping(100, verbose=False)])

            preds = model_lgbm.predict_proba(X_val_final)[:, 1]
            oof_predictions_trial[val_idx] = preds

            # --- [æ ¸å¿ƒä¿®æ”¹] â€œèµ›æ®µâ€å‰ªæ ---
            # åœ¨æ¯æŠ˜ç»“æŸåï¼Œè®¡ç®—å½“å‰OOFé¢„æµ‹çš„ç´¯ç§¯AUCï¼Œå¹¶å‘Optunaæ±‡æŠ¥
            # è¿™æ ·Optunaå°±å¯ä»¥æ ¹æ®å‰å‡ æŠ˜çš„ç»¼åˆè¡¨ç°ï¼Œæ¥å†³å®šæ˜¯å¦æå‰ä¸­æ­¢è¿™ä¸ªæ²¡æœ‰å¸Œæœ›çš„è¯•éªŒ
            if fold < len(folds_to_use) - 1: # ä¸åœ¨æœ€åä¸€æŠ˜æ±‡æŠ¥ï¼Œå› ä¸ºæœ€ç»ˆåˆ†æ•°ä¼šåœ¨å‡½æ•°æœ«å°¾è¿”å›
                temp_valid_indices = np.where(oof_predictions_trial != 0)[0]
                if len(temp_valid_indices) > 0:
                    intermediate_score = roc_auc_score(y.iloc[temp_valid_indices], oof_predictions_trial[temp_valid_indices])
                    trial.report(intermediate_score, fold)
                
                if trial.should_prune():
                    raise optuna.TrialPruned()

        # ... [æœ€ç»ˆOOFåˆ†æ•°è®¡ç®—é€»è¾‘æ— å˜åŒ–] ...
        valid_indices = np.where(oof_predictions_trial != 0)[0]
        if len(valid_indices) == 0: return 0.5
        final_score = roc_auc_score(y.iloc[valid_indices], oof_predictions_trial[valid_indices])
        return final_score

    if tune_target == 'lgbm':
        print(f"\n--- å¯åŠ¨ 'TUNE_LGBM' æ¨¡å¼ ({config.N_TRIALS_LGBM}æ¬¡å°è¯•) ---")
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=config.N_TRIALS_LGBM)
        params_file = config.LGBM_PARAMS_FILE
    elif tune_target == 'ae':
        print(f"\n--- å¯åŠ¨ 'TUNE_AE' æ¨¡å¼ ({config.N_TRIALS_AE}æ¬¡å°è¯•) ---")
        try:
            with open(config.LGBM_PARAMS_FILE, 'r') as f: pass
        except FileNotFoundError:
            print(f"âŒ é”™è¯¯: æ— æ³•å¯åŠ¨AEè°ƒä¼˜, '{config.LGBM_PARAMS_FILE}' ä¸å­˜åœ¨ã€‚è¯·å…ˆè¿è¡Œ 'tune_lgbm'ã€‚")
            return
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=config.N_TRIALS_AE)
        params_file = config.AE_PARAMS_FILE

    print(f"\n{'='*25} Optunaæœç´¢ç»“æŸ {'='*25}")
    print(f"âœ… æ‰¾åˆ°äº† {tune_target.upper()} çš„æœ€ä¼˜å‚æ•°ï¼æœ€ä½³å…¨å±€ OOF AUC: {study.best_value:.8f}")
    print("æœ€ä¼˜å‚æ•°ç»„åˆ:")
    print(json.dumps(study.best_params, indent=4))
    
    with open(params_file, 'w') as f:
        json.dump(study.best_params, f, indent=4)
    print(f"\n-> æœ€ä½³å‚æ•°å·²ä¿å­˜è‡³ '{params_file}'")

# --- 3. ä½œæˆ˜æ¨¡å¼: æœ€ç»ˆéªŒè¯ ---
def run_validation(X, y, sample_weight, date_ids):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("\n--- å¯åŠ¨ 'VALIDATE' æ¨¡å¼: ä½¿ç”¨æœ€ä¼˜å‚æ•°è¿›è¡Œæœ€ç»ˆæ€§èƒ½è¯„ä¼° ---")
    try:
        with open(config.LGBM_PARAMS_FILE, 'r') as f: lgbm_params = json.load(f)
        lgbm_params.update({'objective': 'binary', 'metric': 'auc', 'random_state': 42, 'n_jobs': -1, 'verbosity': -1})
        with open(config.AE_PARAMS_FILE, 'r') as f: ae_params = json.load(f)
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯: ç¼ºå°‘å‚æ•°æ–‡ä»¶: {e}ã€‚è¯·ç¡®ä¿å·²æˆåŠŸè¿è¡Œ 'tune_lgbm' å’Œ 'tune_ae'ã€‚")
        return

    print("âœ… æ‰€æœ‰æœ€ä¼˜å‚æ•°åŠ è½½æˆåŠŸã€‚å¼€å§‹æœ€ç»ˆçš„5æŠ˜äº¤å‰éªŒè¯...")
    tscv = utils.get_cv_splitter(config.N_SPLITS, config.EMBARGO_SIZE, config.PURGE_SIZE)
    oof_predictions = np.zeros(len(X))
    
    # --- [AIç‰¹å¾å®¡æŸ¥ 1/3] åˆ›å»ºä¸€ä¸ªåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨æ¯ä¸€æŠ˜çš„ç‰¹å¾é‡è¦æ€§ ---
    all_feature_importances = []
    
    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
        print(f"\n{'='*20} æ­£åœ¨å¤„ç†ç¬¬ {fold + 1}/{config.N_SPLITS} æŠ˜ {'='*20}")
        purged_train_idx = train_idx[:-config.PURGE_SIZE] if config.PURGE_SIZE > 0 else train_idx
        X_train, y_train = X.iloc[purged_train_idx], y.iloc[purged_train_idx]
        sw_train = sample_weight.iloc[purged_train_idx]
        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]

        # [é‡æ„] å°†Pandasåˆ°Numpyçš„è½¬æ¢ä¹Ÿç§»åˆ°è¿™é‡Œï¼Œä¿æŒä¸tuneæ¨¡å¼ä¸€è‡´
        train_mask_np = X_train.isnull().values
        X_train_processed, X_val_processed = utils.preprocess_for_lgbm(X_train, X_val)
        
        X_train_np, y_train_np = X_train_processed.values, y_train.values
        sw_train_np, X_val_np = sw_train.values, X_val_processed.values
        y_val_np = y_val.values

        ae_models = utils.train_fold_ae(ae_params, X_train_np, y_train_np, sw_train_np, X_val_np, y_val_np, train_mask_np)
        
        if not ae_models:
            print(f"  > è­¦å‘Š: ç¬¬ {fold + 1} æŠ˜æœªèƒ½æˆåŠŸè®­ç»ƒä»»ä½•AEæ¨¡å‹ï¼Œå°†è·³è¿‡æ­¤æŠ˜çš„é¢„æµ‹ã€‚")
            continue

        with torch.no_grad():
            mean, std = np.mean(X_train_np, axis=0), np.std(X_train_np, axis=0)
            std[std==0] = 1.0
            
            train_scaled = torch.tensor((X_train_np - mean) / std, dtype=torch.float32).to(device)
            val_scaled = torch.tensor((X_val_np - mean) / std, dtype=torch.float32).to(device)
            
            all_train_ai, all_val_ai = [], []
            for ae_model in ae_models:
                all_train_ai.append(ae_model.encoder(train_scaled).cpu().numpy())
                all_val_ai.append(ae_model.encoder(val_scaled).cpu().numpy())
            
            X_train_ai, X_val_ai = np.mean(all_train_ai, axis=0), np.mean(all_val_ai, axis=0)

        ai_feature_names = [f'{config.AI_PREFIX}{i}' for i in range(X_train_ai.shape[1])]
        X_train_ai_df = pd.DataFrame(X_train_ai, columns=ai_feature_names, index=X_train_processed.index)
        X_val_ai_df = pd.DataFrame(X_val_ai, columns=ai_feature_names, index=X_val_processed.index)
        X_train_combined = pd.concat([X_train_processed, X_train_ai_df], axis=1)
        X_val_combined = pd.concat([X_val_processed, X_val_ai_df], axis=1)
        
        X_train_final, X_val_final = utils.preprocess_for_lgbm(X_train_combined, X_val_combined)

        model_lgbm = lgb.LGBMClassifier(**lgbm_params)
        model_lgbm.fit(X_train_final, y_train, sample_weight=sw_train,
                       eval_set=[(X_val_final, y_val)],
                       callbacks=[lgb.early_stopping(100, verbose=False)])

        oof_predictions[val_idx] = model_lgbm.predict_proba(X_val_final)[:, 1]

        # --- [AIç‰¹å¾å®¡æŸ¥ 2/3] æå–å¹¶å­˜å‚¨å½“å‰æŠ˜çš„ç‰¹å¾é‡è¦æ€§ ---
        fold_importances = pd.Series(model_lgbm.feature_importances_, index=X_train_final.columns)
        all_feature_importances.append(fold_importances)
        print(f"  > [å®¡æŸ¥] ç¬¬ {fold + 1} æŠ˜ç‰¹å¾é‡è¦æ€§å·²è®°å½•ã€‚")

    # --- [AIç‰¹å¾å®¡æŸ¥ 3/3] è®¡ç®—ã€æ˜¾ç¤ºå¹¶ä¿å­˜å¹³å‡ç‰¹å¾é‡è¦æ€§ ---
    if all_feature_importances:
        avg_importances = pd.concat(all_feature_importances, axis=1).mean(axis=1)
        avg_importances.sort_values(ascending=False, inplace=True)
        
        importance_file = 'feature_importance_with_ai.csv'
        avg_importances.to_csv(importance_file)
        
        print(f"\n{'='*25} AIç‰¹å¾å®¡æŸ¥æŠ¥å‘Š {'='*25}")
        print("æ‰€æœ‰ç‰¹å¾ï¼ˆåŸå§‹+AIï¼‰çš„å¹³å‡é‡è¦æ€§æ’åï¼ˆTop 30ï¼‰:")
        print(avg_importances.head(30))
        print(f"\nâœ… å®Œæ•´çš„ç‰¹å¾é‡è¦æ€§æ’åå·²ä¿å­˜è‡³ '{importance_file}'")


    valid_indices = np.where(oof_predictions != 0)[0]
    final_score = roc_auc_score(y.iloc[valid_indices], oof_predictions[valid_indices])
    
    print(f"\n\n{'='*25} æœ€ç»ˆéªŒè¯ç»“æŸ {'='*25}")
    print(f"ğŸ† æœ€ç»ˆçš„ã€å¯ä¿¡çš„ OOF AUC: {final_score:.8f}")
    
    oof_df = pd.DataFrame({
        'date_id': date_ids['date_id'].iloc[valid_indices],
        'target': y.iloc[valid_indices],
        'oof_prediction': oof_predictions[valid_indices]
    })
    oof_df.to_csv(config.OOF_OUTPUT_FILE, index=False)
    print(f"âœ… OOFé¢„æµ‹ç»“æœå·²ä¿å­˜è‡³ '{config.OOF_OUTPUT_FILE}'ã€‚")
# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ ä½œæˆ˜å¹³å° V1.5 (æœ€ç»ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆ)")
    parser.add_argument('--mode', type=str, required=True, 
                        choices=['rfe', 'tune_lgbm', 'tune_ae', 'validate'],
                        help="é€‰æ‹©è¦æ‰§è¡Œçš„ä½œæˆ˜æ¨¡å¼")
    args = parser.parse_args()

    start_time = time.time()
    
    X, y, sample_weight, date_ids = utils.load_data(
        config.RAW_DATA_FILE, config.ANALYSIS_START_DATE_ID, config.MISSING_THRESHOLD
    )
    
    if args.mode == 'rfe':
        run_rfe(X, y)
    elif args.mode == 'tune_lgbm':
        run_tuning(X, y, sample_weight, 'lgbm')
    elif args.mode == 'tune_ae':
        run_tuning(X, y, sample_weight, 'ae')
    elif args.mode == 'validate':
        run_validation(X, y, sample_weight, date_ids)

    total_time = time.time() - start_time
    print(f"\nä»»åŠ¡ '{args.mode}' å®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f} ç§’ã€‚")