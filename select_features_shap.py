# select_features_shap.py
# =================================================================
# ç»ˆæç‰¹å¾ç­›é€‰è„šæœ¬ V1.2 (ç»ˆæé˜²å¾¡ç‰ˆ)
# =================================================================
import argparse
import time
import pandas as pd
import numpy as np
import lightgbm as lgb
import shap
import importlib
import warnings

warnings.filterwarnings('ignore')

def run_shap_selection(config, n_features_to_select):
    """
    æ‰§è¡ŒåŸºäºSHAPçš„ç‰¹å¾ç­›é€‰æµç¨‹
    """
    print("--- 1. åŠ è½½ä¾èµ–æ¨¡å— (utils.py) ---")
    import utils

    print("\n--- 2. åŠ è½½æ•°æ® ---")
    dev_df = utils.load_data(config.RAW_DATA_FILE, config.ANALYSIS_START_DATE_ID)
    
    all_features = [c for c in dev_df.columns if c.startswith(('M', 'E', 'I', 'P', 'V', 'S', 'D')) or '_diff' in c or '_rol_' in c]
    print(f"  > å°†å¯¹å…¨éƒ¨ {len(all_features)} ä¸ªç‰¹å¾è¿›è¡ŒSHAPé‡è¦æ€§è¯„ä¼°ã€‚")

    X = dev_df[all_features]
    y = dev_df[config.TARGET_COLUMNS]
    
    if 'sample_weight' in dev_df.columns:
        sample_weight = dev_df['sample_weight']
        print("  > âœ… æˆåŠŸåŠ è½½æ ·æœ¬æƒé‡ã€‚")
    else:
        print("  > âŒ é”™è¯¯: æœªåœ¨æ•°æ®ä¸­æ‰¾åˆ° 'sample_weight' åˆ—ã€‚")
        return

    y_primary = y[config.PRIMARY_TARGET_COLUMN]

    print("\n--- 3. è®¾ç½®äº¤å‰éªŒè¯ (CV) ---")
    tscv = utils.get_cv_splitter(config.N_SPLITS, config.EMBARGO_SIZE, config.PURGE_SIZE)
    print(f"  > âœ… CVåˆ†å‰²å™¨å·²è®¾ç½®: {config.N_SPLITS} æŠ˜...")

    print("\n--- 4. å¼€å§‹äº¤å‰éªŒè¯ä¸SHAPå€¼è®¡ç®— ---")
    all_shap_df_list = [] 

    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
        print(f"  > æ­£åœ¨å¤„ç†ç¬¬ {fold + 1}/{config.N_SPLITS} æŠ˜...")
        
        purged_train_idx = train_idx[:-config.PURGE_SIZE] if config.PURGE_SIZE > 0 else train_idx
        X_train, y_train = X.iloc[purged_train_idx], y_primary.iloc[purged_train_idx]
        sw_train = sample_weight.iloc[purged_train_idx]
        X_val, y_val = X.iloc[val_idx], y_primary.iloc[val_idx]

        preprocessor_params = utils.get_preprocessor_params(X_train)
        _, X_train_scaled = utils.apply_preprocessor(X_train, preprocessor_params)
        _, X_val_scaled = utils.apply_preprocessor(X_val, preprocessor_params)

        model = lgb.LGBMClassifier(
            objective='binary', metric='auc', random_state=42, n_estimators=200,
            learning_rate=0.05, num_leaves=8, max_depth=3, reg_alpha=10,
            reg_lambda=10, colsample_bytree=0.5, n_jobs=-1, verbosity=-1
        )
        
        model.fit(X_train_scaled, y_train, sample_weight=sw_train)

        explainer = shap.TreeExplainer(model)
        shap_values = explainer.shap_values(X_val_scaled)
        
        # --- [ç»ˆææ ¸å¿ƒä¿®å¤] ---
        # å¢åŠ é˜²å¾¡æ€§æ£€æŸ¥ã€‚å½“æ¨¡å‹å®Œå…¨å­¦ä¸åˆ°ä¸œè¥¿æ—¶ï¼Œshap_valueså¯èƒ½ä¸æ˜¯ä¸€ä¸ªlistï¼Œæˆ–è€…listé•¿åº¦ä¸ä¸º2
        shap_values_for_class_1 = None
        if isinstance(shap_values, list) and len(shap_values) == 2:
            # è¿™æ˜¯æ­£å¸¸æƒ…å†µï¼Œæˆ‘ä»¬å–ç±»åˆ«1çš„SHAPå€¼
            shap_values_for_class_1 = shap_values[1]
        else:
            # è¿™æ˜¯å¼‚å¸¸æƒ…å†µï¼Œå¯èƒ½æ¨¡å‹å˜æˆäº†â€œå“‘å·´â€æ¨¡å‹ã€‚
            # æ­¤æ—¶ shap_values å¯èƒ½ç›´æ¥å°±æ˜¯æˆ‘ä»¬éœ€è¦çš„é‚£ä¸ªçŸ©é˜µã€‚
            # æˆ‘ä»¬åšä¸€ä¸ªå½¢çŠ¶æ£€æŸ¥æ¥ç¡®è®¤ã€‚
            if shap_values.shape == X_val_scaled.shape:
                 shap_values_for_class_1 = shap_values
            else:
                print(f"    âš ï¸ è­¦å‘Š: ç¬¬ {fold + 1} æŠ˜çš„SHAPå€¼å½¢çŠ¶å¼‚å¸¸ï¼Œå·²è·³è¿‡ã€‚")
                print(f"       æœŸæœ›å½¢çŠ¶: {X_val_scaled.shape}, å®é™…å½¢çŠ¶: {shap_values.shape}")
                continue # è·³è¿‡è¿™ä¸€æŠ˜

        shap_df = pd.DataFrame(shap_values_for_class_1, columns=X_val_scaled.columns)
        all_shap_df_list.append(shap_df)
        # --- [ä¿®å¤ç»“æŸ] ---

    if not all_shap_df_list:
        print("\nâŒ ä¸¥é‡é”™è¯¯: æ‰€æœ‰æŠ˜çš„SHAPå€¼è®¡ç®—å‡å¤±è´¥ã€‚æ— æ³•ç”Ÿæˆç‰¹å¾æ’åã€‚")
        return

    print("\n--- 5. æ±‡æ€»SHAPå€¼å¹¶ç”Ÿæˆæœ€ç»ˆæ’å ---")

    shap_importance_df = pd.concat(all_shap_df_list)
    feature_importance = shap_importance_df.abs().mean().sort_values(ascending=False)

    print(f"\nâœ… SHAPç‰¹å¾é‡è¦æ€§è®¡ç®—å®Œæˆï¼")
    print("=" * 60)
    print("Top 10 æœ€é‡è¦çš„ç‰¹å¾:")
    print(feature_importance.head(10))
    print("=" * 60)
    print("Top 10 æœ€ä¸é‡è¦çš„ç‰¹å¾:")
    print(feature_importance.tail(10))
    print("=" * 60)

    top_features = feature_importance.head(n_features_to_select).index.tolist()

    new_ranking_filename = f"feature_ranking_shap_top{n_features_to_select}.csv"
    pd.Series(top_features).to_csv(new_ranking_filename, index=False, header=False)

    print(f"\nğŸ† å·²é€‰å‡º Top {n_features_to_select} ä¸ªç‹ç‰Œç‰¹å¾ï¼")
    print(f"   åˆ—è¡¨å·²ä¿å­˜è‡³: '{new_ranking_filename}'")
    print("   ä½ ç°åœ¨å¯ä»¥åœ¨ config.py ä¸­å°† RANKING_FILE æ›´æ–°ä¸ºæ­¤æ–‡ä»¶åï¼Œ")
    print(f"   å¹¶å°† N_TOP_FEATURES_TO_USE æ›´æ–°ä¸º {n_features_to_select}ï¼Œç„¶åé‡æ–°è¿è¡Œä½ çš„è¯Šæ–­æˆ–å®Œæ•´æµç¨‹ã€‚")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="ç»ˆæç‰¹å¾ç­›é€‰è„šæœ¬ V1.2 (ç»ˆæé˜²å¾¡ç‰ˆ)")
    parser.add_argument('--config', type=str, default='config', help="é…ç½®æ–‡ä»¶")
    parser.add_argument('--n_select', type=int, default=50, help="æœ€ç»ˆé€‰æ‹”å‡ºçš„ç‹ç‰Œç‰¹å¾æ•°é‡")
    args = parser.parse_args()

    try:
        config_module = importlib.import_module(args.config)
        print(f"--- é…ç½®æ–‡ä»¶ '{args.config}.py' åŠ è½½æˆåŠŸ ---")
    except ImportError:
        print(f"âŒ é”™è¯¯: æ— æ³•æ‰¾åˆ°é…ç½®æ–‡ä»¶ '{args.config}.py'ã€‚"); exit()

    start_time = time.time()
    run_shap_selection(config_module, args.n_select)
    print(f"\nä»»åŠ¡å®Œæˆï¼æ€»è€—æ—¶: {time.time() - start_time:.2f} ç§’ã€‚")